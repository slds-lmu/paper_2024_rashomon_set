\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage[round]{natbib}
\usepackage{todonotes}
\usepackage{csquotes}
\let\gxi=\xi
\input{latex-math-combined.tex}
\geometry{
    a4paper,
    margin=2.5cm
}
\usepackage{xspace}
\newcommand{\TruVar}{\textsc{TruVar}\xspace}
\newcommand{\TruVarImp}{\textsc{TruVarImp}\xspace}
\newcommand{\xbar}{\mbox{$\overline{\mathbf{x}}$}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\deltabar}{\overline{\delta}}
\newcommand{\cset}{\mathcal{X}}
\newcommand{\cmax}{c_{\mathrm{max}}}

\title{Algorithm Description III}
\author{}
\date{}

\begin{document}
\maketitle

To efficiently determine which of a set of sampled hyperparameter configurations belong to the Rashomon set, we adapt the \TruVar algorithm of \citet{truvar2024} to work with implicitly defined threshold levels, following ideas from \citet{lse}, and thus define \TruVarImp.

The \TruVar algorithm keeps track of a set of yet unclassified candidates and chooses configurations to evaluate based on which most reduce the posterior uncertainty, of all candidates, to below a narrowing cutoff variance.
Candidates with posterior distributions that overwhelmingly fall to one side of the threshold are classified and removed from the active set.
The necessary adjustment for implicitly defined threshold levels in \TruVarImp is to keep track of a second set of candidates that narrow down the uncertainty about the optimum, and thereby the threshold.

\paragraph{Setup} {
    As in \citet{truvar2024}, we model the objective function $f(\xv)$ as a Gaussian process \citep{rasmussen2006} with constant mean $\mu_0$, kernel function $\kxxp$ and (in general) heteroscedastic measurement noise $\varepsilon(\xv)\sim \normal(0, \sigma^2(\xv))$, where we observe $y=f(\xv)+\varepsilon(\xv)$.
    Using the kernel matrix $\Kmat_t = \bigl[k(\mathbf{x}_i, \mathbf{x}_j)\bigr]_{i,j=1}^t$ for previously observed $\xv$, the white noise term $\mathbf{\Sigma}_t = \diag(\sigma^2(\xv_1),\ldots,\sigma^2(\xv_t))$, and cross-covariance vector $\kv_t(\xv) = [k(\xv_i, \xv)]_{i=1}^t$, the posterior distribution of $f(\xv_{t+1})$ is thus $\normal(\mu(\xv_{t+1}), \sigma^2(\xv_{t+1}))$, where
    \begin{align}
        \mu_t(\xv_{t+1}) &= \kv_t(\xv_{t+1})^\top(\Kmat_t+\mathbf{\Sigma}_t)^{-1}\mathbf{y}_{1:t}\label{eq:mu_update}\\
        \sigma_t^2(\xv_{t+1}) &= k(\xv_{t+1}, \xv_{t+1}) - \kv_t(\xv_{t+1})^\top(\Kmat_t+\mathbf{\Sigma}_t)^{-1}\kv_t(\xv_{t+1})\mathrm{.}\label{eq:sigma_update}
    \end{align}
    We use the notation of \citet{truvar2024} of $\sigma_{t-1|\xv}^2(\overline{\xv})$ for the posterior variance of $f(\overline{\xv})$ given the observations of $\xv_1,\ldots,\xv_{t-1}$ as well as $\xv$.
    Notably, it does not depend on the observed value of $y$ at $\xv$ and can be calculated relatively efficiently as described in \citet{truvar2024}, Appendix B.

    In practice, the kernel hyperparameters of the Gaussian process are usually not known and estimated from the data, meaning that a different kernel function is used at every step.
    We calculate $\sigma_{t-1|\xv}^2(\overline{\xv})$ with the kernel function as estimated from $\{(\xv_i, y_i)\}_{i=1}^{t-1}$ as an approximation.
}

\paragraph{\TruVarImp algorithm}{
    The algorithm is given in Algorithm~\ref{alg:truvarimp}.
    It samples configurations to be evaluated so that points from a finite input set $\cset$ are classified to be above or below a threshold level $h$.
    We consider the case of minimization, i.e.\ finding a Rashomon set with a threshold defined in terms of the (unknown) minimum of the objective function: $f_{\mathrm{min}} = \min_{x \in \cset} f(x)$.
    We set $h = (1 + \epsilon_{\mathrm{rel}}) f_{\mathrm{min}} + \epsilon_{\mathrm{abs}}$, where we allow the expression of the Rashomon set in terms of relative ($\epsilon_{\mathrm{rel}} \ge 0$; assuming $\forall \xv: f(\xv) \ge 0$) or absolute ($\epsilon_{\mathrm{abs}} \ge 0$) offset from the minimum.
    Typically, one of these would be zero; we put both variables in this formula to remain general.

    \TruVarImp keeps track of a set of potential minimizers $M_t$, a set of values classified as below the threshold $L_t$, a set of unclassified values $U_t$, and a set of values classified as above the threshold $H_t$.
    $L_t$, $U_t$ and $H_t$ form a partition of $\cset$, and $M_t \subseteq U_t \cup L_t$.

    For each point, the algorithm calculates confidence intervals $[l_t(\xv), u_t(\xv)]$, defined as
    \begin{equation}
        l_t(\xv) = \mu_t(\xv) - \beta_{(i)}^{1/2}\sigma_t(\xv)\mathrm{,} \quad \mathrm{and} \quad
        u_t(\xv) = \mu_t(\xv) + \beta_{(i)}^{1/2}\sigma_t(\xv)\mathrm{.}\label{eq:confint}
    \end{equation}
    $\xv$ is removed from $M_t$ whenever its confidence interval does not overlap with the pessimistic estimate of the minimum $f_{\mathrm{min},t}^\mathrm{pes}$, and it is removed from $U_t$ and added to either $L_t$ or $H_t$ whenever its confidence interval does not overlap with the range of possible threshold values $[h_t^\mathrm{opt}, h_t^\mathrm{pes}]$. % making the onscious choice here to define f_min^pes, h^opt|pes in the algorithm, not here.

    The algorithm proceeds by choosing configurations to evaluate based on which most reduces the scaled variance of posterior distributions of candidates in both $M_t$ and $U_t$ to below the target width $\eta_{(i)}$.
    Both the scaling parameter $\beta_{(i)}$ and the confidence interval target width $\eta_{(i)}$ depend on an epoch number $i$, which counts up whenever all confidence intervals fall below a threshold proportional to $\eta_{(i)}$.
    The factor of proportionality depends on whether a candidate is in $M_t$ or $U_t$.

    We define the \emph{total scaled escess variance} $\Delta_t$ as
    \begin{equation}
        \Delta_t(D, \sigma^2, \beta_{(i)}, \eta_{(i)}, p) = \sum_{\xbar \in D}\max\left\{ p \beta_{(i)} \sigma^2(\xbar) - \eta_{(i)}^2, 0 \right\}\mathrm{.}
    \end{equation}
    The scaling factor $p$ is 1 for candidates in $U_t$ and $p = 1 + \epsilon_{\mathrm{rel}}$ for candidates in $M_t$ to account for the latter having a potentially larger influence on the uncertainty about whether candidates are in the Rashomon set.
}

\begin{algorithm}
	\caption{Truncated Variance Reduction for Implicitly Defined Level Set Estimation(\TruVarImp)} \label{alg:truvarimp}
	\begin{algorithmic}[1]
		\Require Domain $\cset$, sampling cost $c(\xv)$, GP prior ($\mu_0$, $\sigma(\xv)$, $\kxxp$), confidence bound parameters $\deltabar > 0$, $r \in (0,1)$, $\{\beta_{(i)}\}_{i \ge 1}$, $\eta_{(1)} > 0$, $\epsilon_{\mathrm{rel}} \ge 0$, $\epsilon_{\mathrm{abs}} \ge 0$
		\State Initialize $L_0=H_0=\emptyset$, $M_0=U_0=\cset$, epoch number $i=1$
		\For {$t = 1,2,\dotsc$}
		\State  Choose
            \begin{equation}
                \begin{split}
                    \hspace*{-1.5ex}  \xv_t = \argmax_{\xv \in D} \Big\{&\Bigl(\Delta_t(M_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1) - \Delta_t(M_{t-1}, \sigma_{t-1|\xv}^2, \beta_{(i)}, \eta_{(i)}, 1) + \\
                    &\Delta_t(H_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}}) - \Delta_t(H_{t-1}, \sigma_{t-1|\xv}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}})\Bigr) / c(\xv) \Big\}
                \end{split}
            \end{equation}
		\State Evaluate the objective function at $\xv_t$ and observe noisy sample $y_t$.
        \State Calculate $\mu_t$ and $\sigma_t$ according to \eqref{eq:mu_update}--\eqref{eq:sigma_update}
        \State Calculate $l_t(\xv)$, and $u_t(\xv)$ according to \eqref{eq:confint}
        \State $h_t^\mathrm{opt} = \min_{\xbar \in M_{t-1}} l_t(\xbar) (1 + \epsilon_{\mathrm{rel}}) + \epsilon_{\mathrm{abs}}$
        \State $f_{\mathrm{min},t}^\mathrm{pes} = \max_{\xbar \in M_{t-1}} u_t(\xbar)$
        \State $h_t^\mathrm{pes} = f_{\mathrm{min},t}^\mathrm{pes} (1 + \epsilon_{\mathrm{rel}}) + \epsilon_{\mathrm{abs}}$
        \State $H_t \leftarrow H_{t-1}$, $L_t \leftarrow L_{t-1}$

        \For {each $\xv \in U_{t-1}$}
            \If {$u_t(\xv) \le h_t^\mathrm{opt}$}
                \State $L_t \leftarrow L_t \cup \{\xv\}$
            \ElsIf {$l_t(\xv) > h_t^\mathrm{pes}$}
                \State $H_t \leftarrow H_t \cup \{\xv\}$
            \Else
                \State $U_t \leftarrow U_t \cup \{\xv\}$
            \EndIf
        \EndFor
        \For {each $\xv \in M_{t-1}$}
            \If {$l_t(\xv) \le h_t^\mathrm{opt}$}
                \State $M_t \leftarrow M_t \cup \{\xv\}$
            \EndIf
        \EndFor

		\While{$\max_{\xv \in U_t} \beta_{(i)}^{1/2}\sigma_{t}(\xv) \le (1+\deltabar) \eta_{(i)}$ and $\max_{\xv \in M_t} \beta_{(i)}^{1/2}\sigma_{t}(\xv) \le \frac{1+\deltabar}{1 + \epsilon_{\mathrm{rel}}} \eta_{(i)}$}
			\State $i \leftarrow i+1$, $\eta_{(i)} = r \times \eta_{(i-1)}$.
		\EndWhile
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Theoretical Analysis}

Our theoretical analysis generally follows \citet{truvar2024} with the necessary adjustments for the modified acquisition function.

Just like them, we define for a collection of points $S$, possibly with duplicates:
\begin{itemize}
    \item the total cost as $c(S) = \sum_{\xv \in S} c(\xv)$, and
    \item the posterior variance at $\xv$ after observing points $\xv_1, \ldots, \xv_t$ as well as new points in $S$ as $\sigma_{t|S}^2(\xv)$.
\end{itemize}

We furthermore let $C^{*}(\gxi, M) = \min_{S} \left\{ \sum_{x \in S} c(x) : \max_{\xv \in M} \sigma_{0|S}^2(\xv) \le \gxi \right\}$ be the minimum cost of a set $S \subseteq M$ that attains a posterior standard deviation of at most $\gxi$ at all points in $M$.

For the purpose of our analysis, we define $\epsilon$-accuracy as follows:

\emph{The triplet $(L_t, H_t, M_t)$ is $\epsilon$-accurate if all $\xv$ in $L_t$ satisfy $f(\xv) < h$, all $\xv$ in $H_t$ satisfy $f(\xv) > h$, and all $\xv$ in $M_t$ satisfy $|h - f(\xv)| \le \epsilon$.}


\paragraph{Theorem.} For a given $\epsilon > 0$ and $\delta \in (0,1)$, and given values $\left\{C_{(i)}\right\}_{i\ge 1}$ and $\left\{\beta_{(i)}\right\}_{i\ge 1}$ that satisfy

\begin{equation}
    \sum_{i} C_{(i)} \le C_{\max} \quad \text{and} \quad \max_{i} \beta_{(i)} \le \beta_{\max}
\end{equation}

Analogously to Appendix~C of \citet{truvar2024}, we let
\begin{eqnarray*}
    g_t(S) & = &\bigl(\Delta (M_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1) - \Delta (M_{t-1}, \sigma_{t-1|S}^2, \beta_{(i)}, \eta_{(i)}, 1) + \\
    && \Delta (H_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}}) - \Delta (H_{t-1}, \sigma_{t-1|S}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}})\bigr) / \beta_{(i)}
    \mathrm{,}
\end{eqnarray*}
and
\begin{equation}
    g_{t,\mathrm{max}} = \left(\Delta (M_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1) + \Delta (H_{t-1}, \sigma_{t-1}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}})\right) / \beta_{(i)}\mathrm{.}
\end{equation}

As in \citet{truvar2024}, the acquisition function of Algorithm~\ref{alg:truvarimp} at time step $t$ constitutes a greedy optimization step of the submodular covering problem of minimizing $c(S)$ subject to $g_t(S) \ge g_{t,\mathrm{max}}$. Equations (25)--(30) follow analogously, i.e.\




\paragraph{Lemma 1.} Consider for $t = 0, 1, \ldots$ a series of monotone submodular functions $g_t(S)$ with $g_t(\emptyset) = 0$ and a cost function $c(S)$, such that $g_{t}(S) - g_{t+1}(S) \ge 0$ is \emph{also} submodular.
Let $g_{t,\mathrm{max}} = \max_S g_{t}(S)$ and $c(S_t^*) = \min_{S} \left\{ c(S) : g_{t}(S) = g_{t,\mathrm{max}} \right\}$ be the minimum cost of a set $S$ that attains the maximum value of $g_{t}$, with $c(S_{t+1}^*) \le c(S_t^*)$.
Let $\mathbf{X}_t = \{\xv_1, \ldots, \xv_t\}$ be the points chosen by the greedy selection rule $\xv_t = \argmax_{\xv \in \cset} \frac{g_{t}(\mathbf{X}_{t-1} \cup \{\xv\}) - g_{t}(\mathbf{X}_{t-1})}{c(\xv)} $.
Then the cost needed to achieve a value $g_{t}(\mathbf{X}_{t})$ that is at least a proportion $\gamma$ of the maximum value of $g_t$ is at most $-c(S_0^*) \log\gamma$.

\paragraph{Proof.}
Making use of Lemma~1 from \citet{the_budgeted_maximum_covering_problem}, we have:
\begin{equation}
    g_t(\mathbf{X}_t) - g_t(\mathbf{X}_{t-1}) \ge \frac{c(\xv_t)}{c(S_t^*)} \left( g_{t,\mathrm{max}} - g_t(\mathbf{X}_{t-1}) \right)\mathrm{.}
\end{equation}
\begin{equation}
    g_t(\mathbf{X}_t) \ge \frac{c(\xv_t)}{c(S_t^*)} g_{t,\mathrm{max}} + \left(1 - \frac{c(\xv_t)}{c(S_t^*)}\right) g_t(\mathbf{X}_{t-1})\mathrm{.}
\end{equation}


Making use of Lemma~2 from \citet{the_budgeted_maximum_covering_problem}, we have:
\begin{eqnarray*}
    g(\mathbf{X}_t) & \ge & \left(1 - \prod_{i=1}^{t} \left(1 - \frac{c(\xv_i)}{c(S^*)}\right)\right) g_{\mathrm{max}} \quad \Leftrightarrow\\
    \prod_{i=1}^{t} \left(1 - \frac{c(\xv_i)}{c(S^*)}\right) & \ge & \frac{g_{\mathrm{max}} - g(\mathbf{X}_t)}{g_{\mathrm{max}}} \quad \Rightarrow\\
    \exp\left(-\frac{\sum_{i=1}^{t} c(\xv_i)}{c(S^*)}\right) & \ge & \frac{g_{\mathrm{max}} - g(\mathbf{X}_t)}{g_{\mathrm{max}}}\textrm{,}
\end{eqnarray*}
making use of the fact that $1-x \le \exp(-x)$ for $x \ge 0$. Rearranging further, we get
\begin{equation}
    \sum_{i=1}^{t} c(\xv_i) \le c(S^*) \log\left(\frac{g_{\mathrm{max}}}{g_{\mathrm{max}} - g(\mathbf{X}_t)}\right)\textrm{.}
\end{equation}


\paragraph{Lemma.} The cost incurred in epoch $i$ is at most $C_{(i)}$.

\paragraph{Proof.} Use Lemma~1 with
\begin{eqnarray*}
    g(S) & = & \Delta(M_{t_{(i)}-1}, \sigma_{t_{(i)}-1}^2, \beta_{(i)}, \eta_{(i)}, 1) - \Delta(M_{t_{(i)}-1}, \sigma_{t_{(i)}-1|S}^2, \beta_{(i)}, \eta_{(i)}, 1) + \\
                    &&\Delta(H_{t_{(i)}-1}, \sigma_{t_{(i)}-1}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}}) - \Delta(H_{t_{(i)}-1}, \sigma_{t_{(i)}-1|S}^2, \beta_{(i)}, \eta_{(i)}, 1 + \epsilon_{\mathrm{rel}})\textrm{.}
\end{eqnarray*}
This has $g(\emptyset) = 0$ and is monotone submodular, as $\sigma_{t}^2(\xv) - \sigma_{t|S}^2(\xv)$ is monotone submodular, which is preserved by truncation and addition.

Having a normalized kernel function $k(\xv, \xv) = 1$ limits the maximum value of $g(S)$:
\begin{equation}
    g_{\mathrm{max}} = \beta_{(i)}\left(|M_{t_{(i)}-1}| + (1 + \epsilon_{\mathrm{rel}})|H_{t_{(i)}-1}|\right)
\end{equation}









Below is a sketch of how the main arguments and proofs from the original \TruVar algorithm carry over to the \TruVarImp variant for implicitly defined thresholds.
Much of the submodular-variance-reduction machinery remains unchanged.

The key differences are:

\begin{enumerate}
    \item \TruVarImp keeps track of two main sets whose variances must be reduced, rather than a single set $M_t$.
    The set of potential minimizers $M_t$ is scaled differently from the set $U_t$ of unclassified points, reflecting the fact that points in $M_t$ more strongly affect the (unknown) threshold.
    \item \TruVarImp places candidates into $L_t$ or $H_t$ once their confidence intervals lie entirely below or above the threshold range $\bigl[h_t^\mathrm{opt},\,h_t^\mathrm{pes}\bigr]$.
    This threshold range itself depends on the (unknown) minimum $f_{\mathrm{min}}$.
\end{enumerate}
Below we outline how each step of the analysis in the original proofs adapts to these modifications.

\subsection{Valid Confidence Bounds}

As in the original \TruVar analysis, we assume that there exist constants $\{\beta_{(i)}\}$ chosen so that, with probability at least $1-\delta$,
\[
    \bigl\lvert f(\xv) - \mu_t(\xv)\bigr\rvert
    \; \le \;
    \sqrt{\beta_{(i)}}\, \sigma_t(\xv),
    \quad \forall t,\;\forall \xv \in \cset,
\]
where $i$ is the current epoch index at time $t$.
This assumption ensures that
\[
    \ell_t(\xv) \;\;=\;\; \mu_t(\xv) - \sqrt{\beta_{(i)}}\,\sigma_t(\xv),
    \quad
    u_t(\xv) \;\;=\;\; \mu_t(\xv) + \sqrt{\beta_{(i)}}\,\sigma_t(\xv)
\]
are valid lower and upper confidence bounds on $f(\xv)$.
The usual union-bound argument from the standard Gaussian-process confidence bounds (e.g.\ Lemma 5.1 in \citet{Sri12}) applies here as well, with the caveat that the \enquote{time horizon} for each epoch $i$ is bounded by the total cost $C_{(i)}$ in that epoch, rather than the number of rounds.
In other words, one must choose $\beta_{(i)}$ to be large enough to handle all times $t$ such that the cumulative cost up to epoch $i$ does not exceed $C_{(i)}$.
The details are nearly identical to those in the proof of Theorem 1 of the original paper, simply replacing the set $D$ by $\cset$ and ensuring the cost-based time indexing is respected (as in $\sum_{i'\le i} C_{(i')}$ in the original statements).

Once these confidence intervals hold, the algorithm never excludes the true minimizers from $M_t$, nor does it incorrectly label a point in $H_t$ or $L_t$ if in truth it lies on the opposite side of the threshold.

\subsection{Submodular Excess-Variance Reduction}

In \TruVarImp, each epoch $i$ fixes a target width $\eta_{(i)}$.
We have two sets of interest:

\begin{itemize}
    \item $M_t$, the set of potential minimizers, for which we use a scaling factor $1+\epsilon_{\mathrm{rel}}$;
    \item $U_t$, the unclassified set, for which the scaling factor is $1$.
\end{itemize}

Define the \emph{scaled} truncation function:
\[
   \max\{\,p\,\beta_{(i)}\,\sigma^2(\xbar) \;-\; \eta_{(i)}^2,\;0\},
\]
where $p=1$ for $\xbar\in U_t$ and $p=1+\epsilon_{\mathrm{rel}}$ for $\xbar\in M_t$.
In the code, one separately sums over $U_t$ (with $p=1$) and $M_t$ (with $p=1+\epsilon_{\mathrm{rel}}$), but mathematically one can combine them into a single expression by letting each point $\xbar$ carry its own scaling factor $p(\xbar)\in\{1,\,1+\epsilon_{\mathrm{rel}}\}$.

Hence define, for a set $S$ of query points (possibly with duplicates) chosen after time $t-1$,
\[
  \Delta_{t}(S)
  \;\;=\;\;
   \sum_{\xbar\in M_{t-1} \cup U_{t-1}}
   \Bigl[
     \max\{\,p(\xbar)\,\beta_{(i)}\,\sigma_{t-1}^2(\xbar) - \eta_{(i)}^2, \;0\}
     \;-\;
     \max\{\,p(\xbar)\,\beta_{(i)}\,\sigma_{t-1|S}^2(\xbar) - \eta_{(i)}^2, \;0\}
   \Bigr],
\]
where $\sigma_{t-1|S}^2(\xbar)$ denotes the posterior variance of $f(\xbar)$ after also observing all points in $S$.
This is a submodular set function of $S$, by the same argument as in the original \TruVar proof: submodularity of $\sigma^2(\cdot)$ (under Gaussian-process variance reduction) is preserved when we truncate by $\max\{\cdot-\eta_{(i)}^2,0\}$ and sum over points.

\subsection{Greedy Selection per Round}

At time \(t\), \TruVarImp\ selects
\[
  \xv_t
  \;=\;
  \argmax_{\xv \in \cset}
  \frac{
    \Delta_t(\{\xv\})
  }{
    c(\xv)
  },
\]
where $\Delta_t(\{\xv\})$ is precisely the \emph{immediate} reduction in scaled variance (for both sets $M_{t-1}$ and $U_{t-1}$) obtained by querying $\xv$.
This is equivalent to the “first step” of the usual greedy algorithm for maximizing a monotone submodular function subject to a cost budget.
Hence, by Lemma 2 of \citet{Kra05}, one obtains
\[
  \Delta_t(\{\xv_t\})
  \;\;\ge\;\;
  \frac{c(\xv_t)}{c(S_t^*)}\;
  \Delta_t(S_t^*),
\]
where $S_t^*$ is an optimal solution to the (unconstrained) budgeted submodular “cover” problem of saturating all terms $\max\{\dots\}$ to zero (or equivalently, driving every posterior variance term below $\eta_{(i)}^2 / (p(\xbar)\,\beta_{(i)})$).

\subsection{Epoch Completion}

As in the original algorithm, each epoch $i$ ends once $\max_{\xbar \in U_t} \sqrt{\beta_{(i)}}\,\sigma_{t}(\xbar)$ falls below $(1+\deltabar)\eta_{(i)}$ and $\max_{\xbar \in M_t} \sqrt{\beta_{(i)}}\,\sigma_{t}(\xbar)$ falls below $\tfrac{1+\deltabar}{1+\epsilon_{\mathrm{rel}}}\,\eta_{(i)}$.
In terms of the truncated sums, this means the “excess variance” has been saturated down to a factor $\deltabar^2$ times the initial level.
The submodular covering argument therefore bounds the cost incurred in each epoch $i$ by
\[
   c(S_{\mathrm{epoch}\,i}^*)
   \,\log
   \Bigl[
     \tfrac{
       |\,U_{t_{(i)}-1}\cup M_{t_{(i)}-1}|\;\beta_{(i)}
     }{
       \deltabar^2\, \eta_{(i)}^2
     }
   \Bigr]
   \;+\;
   \cmax,
\]
exactly paralleling the derivation of $\sum_{i} C_{(i)}$ in Theorem 1 (the “General Result”) of the original paper.
The only difference is the presence of two different scaling factors $\{1,\;1+\epsilon_{\mathrm{rel}}\}$ in the truncated sums; this modifies only the constants in front of $\eta_{(i)}$, but not the structure of the proof.

Thus, one concludes an overall bound on the cost per epoch, and summing across epochs gives a total cost $C_{\epsilon}$ after which all remaining unclassified points must lie within $\epsilon$-distance of the (unknown) threshold, while all points not in $M_t$ must be suboptimal with high probability.
Moreover, once a point is placed in $H_t$ or $L_t$, the valid confidence intervals imply that this classification is correct.

\subsection{Attaining $\epsilon$-Accuracy with High Probability}

The final step checks the implicit threshold condition:
\[
   h
   \;=\;
   (1 + \epsilon_{\mathrm{rel}})\,f_{\min} + \epsilon_{\mathrm{abs}},
   \qquad
   f_{\min} \;=\; \min_{\xv \in \cset} f(\xv).
\]
Because $h$ depends on $f_{\min}$, \TruVarImp\ maintains an estimate $h_t^\mathrm{opt}$ (optimistic threshold) and $h_t^\mathrm{pes}$ (pessimistic threshold), both refined as variances drop around candidate minimizers.
Points for which $\bigl[u_t(\xv),\, \ell_t(\xv)\bigr]$ falls strictly below $h_t^\mathrm{opt}$ are placed in $L_t$, and those strictly above $h_t^\mathrm{pes}$ go to $H_t$.
By the high-probability event of valid confidence intervals, those decisions match the unknown truth $f(\xv)\gtrless h$.

Concurrently, points are dropped from $M_t$ if they cannot be the minimizer under the confidence intervals.
One shows, as in the original LSE arguments of \citet{Got13}, that once
\[
  \max_{\xv\in M_t}
  \Bigl[\,
    \sqrt{\beta_{(i)}}\,\sigma_t(\xv)
  \Bigr]
  \;\;\le\;\;
  \tfrac{1+\deltabar}{\,1+\epsilon_{\mathrm{rel}}\,}\,\eta_{(i)},
\]
then $\max_{\xv\in M_t}\{u_t(\xv)\}$ and $\min_{\xv\in M_t}\{\ell_t(\xv)\}$ differ by at most $2\,(1+\deltabar)\eta_{(i)}$.
Hence no large “gaps” remain, and either (i) $M_t$ has shrunk to only truly near-minimal configurations or (ii) the next epoch begins with a smaller $\eta_{(i+1)}$.

The same epoch-based summation of costs as in Theorem 1 of the original paper then implies that, once total cost exceeds a bound $C_{\epsilon}$ (depending on $\epsilon$, $\delta$, the GP kernel, and so on), every candidate $\xv$ is known up to $\epsilon$-precision with probability at least $1-\delta$.
In particular, the classification $L_t\cup H_t\cup U_t$ is correct for all $\xv$, and the set $M_t\subseteq U_t\cup L_t$ contains only genuinely near-minimal points.

\subsection{Corollaries for Specific Settings}

All of the corollaries in the original \TruVar analysis—such as homoscedastic noise plus uniform costs, improved noise dependence for small $\sigma^2$, or the “choose-your-noise” scenario—carry over to \TruVarImp with only superficial changes.
One simply replaces:

\begin{itemize}
    \item the single set $M_t$ in the original proofs with the two sets $M_t$ and $U_t$, each having its own scale factor;
    \item any mention of a “discarded” point with “placed in \(H_t\)” or “placed in \(L_t\)” if the threshold intervals do not overlap.
\end{itemize}

The same submodular covering argument and epoch-based analysis applies, yielding cost bounds of the same form
\[
   T \;\;\ge\;
   \Omega^*\!\Bigl(\,\tfrac{\gamma_T\,\beta_T}{\epsilon^2} + 1\Bigr)
   \quad\text{or}\quad
   C_\epsilon \;\;=\;\; O\!\Bigl(
     C^*\bigl(\tfrac{\eta}{\sqrt{\beta}},\,M\bigr)\,\log\!\dots
   \Bigr),
\]
depending on which of the original corollaries one invokes.




\begin{thebibliography}{99}
\bibitem[TruVar(2024)]{truvar2024}
    John Doe.
    \newblock TruVar: Truncated Variance Reduction for Bayesian Optimization
    \newblock {\em Some Journal}, 2024
\bibitem[LSE(2024)]{lse}
    John Doe.
    \newblock LSE: Level Set Estimation
    \newblock {\em Some Journal}, 2024
\bibitem[Rasmussen(2006)]{rasmussen2006}
    Carl Edward Rasmussen.
    \newblock Gaussian Processes for Machine Learning
    \newblock {\em MIT Press}, 2006
\bibitem[Srinivas et al.(2012)]{Sri12}
    Niranjan Srinivas, Andreas Krause, Sham M. Kakade, Matthias Seeger, and Matthias W. Seeger.
    \newblock Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design
    \newblock {\em ICML}, 2012
\bibitem[Krause et al.(2005)]{Kra05}
    Andreas Krause, Ajit Singh, and Carlos Guestrin.
    \newblock Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies
    \newblock {\em JMLR}, 2005
\bibitem[G\"otz et al.(2013)]{Got13}
    G\"otz Pflug, J\"urgen R\"ock, and Wolfgang Stummer.
    \newblock Level set estimation in the presence of noise
    \newblock {\em JMLR}, 2013
\end{thebibliography}

\end{document}
